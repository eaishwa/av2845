{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import random,string\n",
    "import pandas as pd\n",
    "\n",
    "soup=BeautifulSoup(open('cfc_19100108_102.htm'),'html.parser')\n",
    "tables = soup.find_all('table')\n",
    "all_dfs = []\n",
    "\n",
    "for table in tables:\n",
    "\n",
    "    headers=table.find_all('th')\n",
    "    n_col = 0\n",
    "    n_row_tot = 0\n",
    "    \n",
    "    # find number of rows and columns to create a dataframe\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = row.find_all(['td'])\n",
    "        if len(cols)>0:\n",
    "            n_row_tot+=1\n",
    "            if len(cols) > n_col:\n",
    "                n_col = len(cols)\n",
    "    n_row=0\n",
    "    \n",
    "    # find number of rows that the column headers will occupy\n",
    "    for row in table.find_all('tr'):\n",
    "        if len(row.find_all(['th'])) != 0:\n",
    "            n_row = n_row + 1\n",
    "\n",
    "    \n",
    "    skip_index = [0 for i in range(0, n_col)]\n",
    "    total_arr = []\n",
    "    i=0\n",
    "    \n",
    "    # logic to read the column headers and frame the multi index object for pandas df\n",
    "    for m in range(0,n_row):\n",
    "        arr = [None for i in range(0,n_col)]\n",
    "        arr_c=0\n",
    "        while arr_c < n_col:\n",
    "            rs = headers[i].get(\"rowspan\")\n",
    "            cs = headers[i].get(\"colspan\")\n",
    "\n",
    "            if rs is None and cs is None:\n",
    "                if skip_index[arr_c] != 0:\n",
    "                    skip_index[arr_c]=skip_index[arr_c]-1\n",
    "                    arr[arr_c]=''\n",
    "                    arr_c = arr_c + 1\n",
    "                else:\n",
    "                    arr[arr_c]=headers[i].get_text().strip()\n",
    "                    arr_c = arr_c + 1\n",
    "                    if i != len(headers)-1:\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            if rs is not None:\n",
    "                if skip_index[arr_c] == 0:\n",
    "                    skip_index[arr_c] = int(rs)-1\n",
    "                    arr[arr_c]=headers[i].get_text().strip()\n",
    "                    arr_c = arr_c + 1\n",
    "                    if i != len(headers)-1:\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            if cs is not None:\n",
    "                for a in range(arr_c,arr_c+int(cs)):\n",
    "                    arr[a] = headers[i].get_text().strip()\n",
    "                    arr_c=sum(x is not None for x in arr)\n",
    "                if i != len(headers)-1:\n",
    "                        i = i+1\n",
    "                else:\n",
    "                    break\n",
    "            arr_c=sum(x is not None for x in arr)\n",
    "        total_arr.append(arr)\n",
    "\n",
    "    new_table = pd.DataFrame(index = range(0, n_row_tot),columns=pd.MultiIndex.from_arrays(total_arr))\n",
    "    row=0\n",
    "    \n",
    "    # logic to fill data in the df (assumption is that data is available per row per column)\n",
    "    for each_row in table.find_all('tr'):\n",
    "        data = each_row.find_all('td')\n",
    "        if len(data) != 0:\n",
    "            col=0\n",
    "            for cell in data:\n",
    "                new_table.iloc[row,col]=cell.get_text().strip()\n",
    "                col=col+1\n",
    "            row = row+1 \n",
    "    all_dfs.append(new_table)\n",
    "\n",
    "final_dfs=[]\n",
    "\n",
    "# logic to find out which of the tables are identical and merge them into one\n",
    "while len(all_dfs) != 0:\n",
    "    same = []\n",
    "    i=1\n",
    "    m=0\n",
    "    if len(all_dfs) != 1:\n",
    "        for i in range(1,len(all_dfs)):\n",
    "            if all_dfs[0].columns.equals(all_dfs[i].columns):\n",
    "                m=m+1\n",
    "                same.append(all_dfs[0])\n",
    "                same.append(all_dfs[i])\n",
    "    if m==0:\n",
    "        final_dfs.append(all_dfs[0])   \n",
    "    del all_dfs[0]\n",
    "    for a in all_dfs:\n",
    "        if len(same) != 0:\n",
    "            if same[0].columns.equals(a.columns):\n",
    "                all_dfs.remove(a)\n",
    "\n",
    "    if len(same)!=0:\n",
    "        joined_df = pd.concat(same)\n",
    "        final_dfs.append(joined_df)\n",
    "\n",
    " \n",
    "writer=pd.ExcelWriter('output.xlsx')\n",
    "for tab in final_dfs:\n",
    "    tab.to_excel(writer,''.join(random.choice(string.ascii_lowercase) for i in range(5)))\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
